{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most basic stuff for EDA.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Core packages for text processing.\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Libraries for text preprocessing.\n",
    "\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Loading some sklearn packaces for modelling.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Some packages for word clouds and NER.\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import Counter, defaultdict\n",
    "from PIL import Image\n",
    "import spacy\n",
    "import morfeusz2\n",
    "import pl_core_news_md\n",
    "\n",
    "# Core packages for general use throughout the notebook.\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# For customizing our plots.\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Loading pytorch packages.\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Setting some options for general use.\n",
    "\n",
    "stop = set(stopwords.words('polish'))\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set(font_scale=1.5)\n",
    "pd.options.display.max_columns = 250\n",
    "pd.options.display.max_rows = 250\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#Setting seeds for consistent results.\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = r'../../data/raw/train.tsv'\n",
    "STOPWORDS = r'E:\\coding\\pythonnew\\POLHATE-project\\data\\raw\\polish_stopwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dla mnie faworytem do tytułu będzie Cracovia. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@anonymized_account @anonymized_account Brawo ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@anonymized_account @anonymized_account Super,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@anonymized_account @anonymized_account Musi. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Odrzut natychmiastowy, kwaśna mina, mam problem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  target\n",
       "0  Dla mnie faworytem do tytułu będzie Cracovia. ...       0\n",
       "1  @anonymized_account @anonymized_account Brawo ...       0\n",
       "2  @anonymized_account @anonymized_account Super,...       0\n",
       "3  @anonymized_account @anonymized_account Musi. ...       0\n",
       "4    Odrzut natychmiastowy, kwaśna mina, mam problem       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATAPATH, delimiter = '\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Bert Model\n",
    "Finally it's time to start building our model. We gonna use BERT for this task with the help of library called Transformers which makes our work much more smoother..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1060 3GB\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.  \n",
    "    \n",
    "    device = torch.device('cuda')    \n",
    "\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training tweets: 7530\n",
      "\n",
      "Number of training tweets: 2511\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>@anonymized_account Dobry argument żeby zamkną...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4688</th>\n",
       "      <td>imagine, mieszkać w drewnianym domku i w lesie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>@anonymized_account A te slajdy to macie od Mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6434</th>\n",
       "      <td>@anonymized_account @anonymized_account Kim je...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>@anonymized_account Ej zrób mi takie tez xd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  target\n",
       "817   @anonymized_account Dobry argument żeby zamkną...       0\n",
       "4688     imagine, mieszkać w drewnianym domku i w lesie       0\n",
       "1309  @anonymized_account A te slajdy to macie od Mo...       0\n",
       "6434  @anonymized_account @anonymized_account Kim je...       0\n",
       "3084        @anonymized_account Ej zrób mi takie tez xd       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df)\n",
    "print(f'Number of training tweets: {train.shape[0]}\\n')\n",
    "print(f'Number of training tweets: {test.shape[0]}\\n')\n",
    "\n",
    "display(train.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train['target'].values\n",
    "idx = len(labels)\n",
    "combined = pd.concat([train, test])\n",
    "combined = combined.sentence.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Formatting the Inputs\n",
    "For feeding our text to BERT we have to tokenize our text first and then these tokens must be mapped. For this job we gonna download and use BERT's own tokenizer. Thanks to Transformers library it's like one line of code, we also convert our tokens to lowercase for uncased model. You can see how the tokenizer works below there on first row of tweets for example.\n",
    "We set our max len according to our tokenized sentences for padding and truncation, then we use tokenizer.encode_plus it'll split the sentences into tokens, then adds special tokens for classificication [CLS]:\n",
    "The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. (from the BERT paper)\n",
    "\n",
    "Then it adds [SEP] tokens for making BERT decide if sentences are related. In our case it shouldn't be that important I think.\n",
    "Then our tokenizer map's our tokens to their IDs first and pads or truncates all sentences to same length according to our max length. If sentence is longer than our limit it gets truncated, if it's shorter than our defined length then it adds [PAD] tokens to get them in same length.\n",
    "Finally tokenizer create attention masks which is consisting of 1's and 0's for differentiating [PAD] tokens from the actual tokens.\n",
    "We do these steps for each train and test set and then get our converted data for our BERT model. We also split train test on our train data for checking our models accuracy.\n",
    "Lastly we define how to load the data into our model for training, since we can't use it all at once because of memory restrictions. On the official BERT paper batch size of 16 or 32 is recommended so we went with 32 since Kaggle offers us decent GPU's thanks to them!¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = {\n",
    "    \"herbert-klej-cased-v1\": {\n",
    "        \"tokenizer\": \"allegro/herbert-klej-cased-tokenizer-v1\", \n",
    "        \"model\": \"allegro/herbert-klej-cased-v1\",\n",
    "    },\n",
    "    \"herbert-base-cased\": {\n",
    "        \"tokenizer\": \"allegro/herbert-base-cased\", \n",
    "        \"model\": \"allegro/herbert-base-cased\",\n",
    "    },\n",
    "    \"herbert-large-cased\": {\n",
    "        \"tokenizer\": \"allegro/herbert-large-cased\", \n",
    "        \"model\": \"allegro/herbert-large-cased\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the combined text data using bert tokenizer.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_names[\"herbert-klej-cased-v1\"][\"tokenizer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  @anonymized_account @anonymized_account A ci co nie głosowali? Wszyscy jesteście pieprzonymi gnojami.\n",
      "Tokenized:  ['@</w>', 'an', 'ony', 'mi', 'zed</w>', '_</w>', 'ac', 'coun', 't</w>', '@</w>', 'an', 'ony', 'mi', 'zed</w>', '_</w>', 'ac', 'coun', 't</w>', 'A</w>', 'ci</w>', 'co</w>', 'nie</w>', 'głosowali</w>', '?</w>', 'Wszyscy</w>', 'jesteście</w>', 'pieprz', 'onymi</w>', 'gno', 'jami</w>', '.</w>']\n",
      "Token IDs:  [3, 625, 40408, 288, 45935, 50120, 4771, 23590, 117, 3, 625, 40408, 288, 45935, 50120, 4771, 23590, 117, 122, 258, 45, 22, 17672, 42, 4018, 13431, 41167, 1782, 16294, 7502, 15]\n"
     ]
    }
   ],
   "source": [
    "# Print the original tweet.\n",
    "\n",
    "print(' Original: ', combined[0])\n",
    "\n",
    "# Print the tweet split into tokens.\n",
    "\n",
    "print('Tokenized: ', tokenizer.tokenize(combined[0]))\n",
    "\n",
    "# Print the sentence mapped to token ID's.\n",
    "\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(combined[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  95\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "\n",
    "for text in combined:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    \n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    \n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making list of sentence lenghts:\n",
    "\n",
    "token_lens = []\n",
    "\n",
    "for text in combined:\n",
    "    tokens = tokenizer.encode(text)\n",
    "    token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(figsize=(14, 6))\n",
    "sns.distplot(token_lens, color='#e74c3c')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7530,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= combined[:idx]\n",
    "test = combined[idx:]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_map(sentence,labs='None'):\n",
    "    \n",
    "    \"\"\"A function for tokenize all of the sentences and map the tokens to their word IDs.\"\"\"\n",
    "    \n",
    "    global labels\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    \n",
    "    for text in sentence:\n",
    "        #   \"encode_plus\" will:\n",
    "        \n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            text,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            truncation='longest_first', # Activate and control truncation\n",
    "                            max_length = 84,           # Max length according to our text data.\n",
    "                            pad_to_max_length = True, # Pad & truncate all sentences.\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the id list. \n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        \n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n",
    "        labels = torch.tensor(labels)\n",
    "        return input_ids, attention_masks, labels\n",
    "    else:\n",
    "        return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing all of the train test sentences and mapping the tokens to their word IDs.\n",
    "\n",
    "input_ids, attention_masks, labels = tokenize_map(train, labels)\n",
    "test_input_ids, test_attention_masks= tokenize_map(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,024 training samples\n",
      "1,506 validation samples\n"
     ]
    }
   ],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 80-20 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it here. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = TensorDataset(test_input_ids, test_attention_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaModel.from_pretrained(\"allegro/herbert-klej-cased-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-klej-cased-v1 were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-klej-cased-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50560, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_names[\"herbert-klej-cased-v1\"][\"model\"],\n",
    "                                  num_labels = 2, # The number of output labels--2 for binary classification. You can increase this for multi-class tasks.   \n",
    "                                  output_attentions = False, # Whether the model returns attentions weights.\n",
    "                                  output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "roberta.embeddings.word_embeddings.weight               (50560, 768)\n",
      "roberta.embeddings.position_embeddings.weight             (514, 768)\n",
      "roberta.embeddings.token_type_embeddings.weight             (1, 768)\n",
      "roberta.embeddings.LayerNorm.weight                           (768,)\n",
      "roberta.embeddings.LayerNorm.bias                             (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "roberta.encoder.layer.0.attention.self.query.weight       (768, 768)\n",
      "roberta.encoder.layer.0.attention.self.query.bias             (768,)\n",
      "roberta.encoder.layer.0.attention.self.key.weight         (768, 768)\n",
      "roberta.encoder.layer.0.attention.self.key.bias               (768,)\n",
      "roberta.encoder.layer.0.attention.self.value.weight       (768, 768)\n",
      "roberta.encoder.layer.0.attention.self.value.bias             (768,)\n",
      "roberta.encoder.layer.0.attention.output.dense.weight     (768, 768)\n",
      "roberta.encoder.layer.0.attention.output.dense.bias           (768,)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
      "roberta.encoder.layer.0.intermediate.dense.weight        (3072, 768)\n",
      "roberta.encoder.layer.0.intermediate.dense.bias              (3072,)\n",
      "roberta.encoder.layer.0.output.dense.weight              (768, 3072)\n",
      "roberta.encoder.layer.0.output.dense.bias                     (768,)\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight               (768,)\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias                 (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "classifier.dense.weight                                   (768, 768)\n",
      "classifier.dense.bias                                         (768,)\n",
      "classifier.out_proj.weight                                  (2, 768)\n",
      "classifier.out_proj.bias                                        (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples:\n",
    "\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch).\n",
    "\n",
    "# The 'W' stands for 'Weight Decay fix' probably...\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 6e-6, # args.learning_rate\n",
    "                  eps = 1e-8 # args.adam_epsilon\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "\n",
    "# We chose to run for 3, but we'll see later that this may be over-fitting the training data.\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs] (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "Time to train our model! First we set some helper functions to calculate our metrics and time spent on the process. Then it moves like this, directly from the original notebook, it's pretty good at explaining I shouldn't confuse you with my own way of telling I guess:\n",
    "Training:\n",
    "\n",
    "Unpack our data inputs and labels\n",
    "Load data onto the GPU for acceleration,\n",
    "Clear out the gradients calculated in the previous pass,\n",
    "In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out,\n",
    "Forward pass (feed input data through the network),\n",
    "Backward pass (backpropagation),\n",
    "Tell the network to update parameters with optimizer.step(),\n",
    "Track variables for monitoring progress.\n",
    "Evalution:\n",
    "\n",
    "Unpack our data inputs and labels,\n",
    "Load data onto the GPU for acceleration,\n",
    "Forward pass (feed input data through the network),\n",
    "Compute loss on our validation data and track variables for monitoring progress.\n",
    "Pytorch hides all of the detailed calculations from us, but we've commented the code to point out which of the above steps are happening on each line.\n",
    "\n",
    "The code below trains according to our data and saves the learning progress on the way so we can summarize at the end and see our results. We can also turn these to dataframe and plot it to see our eavluation better. So we can decide if the model performs well and not overfitting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    \n",
    "    \"\"\"A function for calculating accuracy scores\"\"\"\n",
    "    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return accuracy_score(labels_flat, pred_flat)\n",
    "\n",
    "def flat_f1(preds, labels):\n",
    "    \n",
    "    \"\"\"A function for calculating f1 scores\"\"\"\n",
    "    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return f1_score(labels_flat, pred_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):    \n",
    "    \n",
    "    \"\"\"A function that takes a time in seconds and returns a string hh:mm:ss\"\"\"\n",
    "    \n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 3.00 GiB total capacity; 2.42 GiB already allocated; 0 bytes free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32me:\\coding\\pythonnew\\POLHATE-project\\src\\notebooks\\3.0-deep-learning-models.ipynb Cell 31\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/coding/pythonnew/POLHATE-project/src/notebooks/3.0-deep-learning-models.ipynb#Y102sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m model\u001b[39m.\u001b[39mzero_grad()        \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/coding/pythonnew/POLHATE-project/src/notebooks/3.0-deep-learning-models.ipynb#Y102sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# Perform a forward pass (evaluate the model on this training batch).\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/coding/pythonnew/POLHATE-project/src/notebooks/3.0-deep-learning-models.ipynb#Y102sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m# The documentation for this `model` function is down here: \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/coding/pythonnew/POLHATE-project/src/notebooks/3.0-deep-learning-models.ipynb#Y102sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers BertForSequenceClassification.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/coding/pythonnew/POLHATE-project/src/notebooks/3.0-deep-learning-models.ipynb#Y102sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/coding/pythonnew/POLHATE-project/src/notebooks/3.0-deep-learning-models.ipynb#Y102sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m# It returns different numbers of parameters depending on what arguments given and what flags are set. For our useage here, it returns the loss (because we provided labels),\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/coding/pythonnew/POLHATE-project/src/notebooks/3.0-deep-learning-models.ipynb#Y102sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m# And the 'logits' (the model outputs prior to activation.)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/coding/pythonnew/POLHATE-project/src/notebooks/3.0-deep-learning-models.ipynb#Y102sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m loss, logits \u001b[39m=\u001b[39m model(b_input_ids, \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/coding/pythonnew/POLHATE-project/src/notebooks/3.0-deep-learning-models.ipynb#Y102sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m                      token_type_ids\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/coding/pythonnew/POLHATE-project/src/notebooks/3.0-deep-learning-models.ipynb#Y102sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m                      attention_mask\u001b[39m=\u001b[39;49mb_input_mask, \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/coding/pythonnew/POLHATE-project/src/notebooks/3.0-deep-learning-models.ipynb#Y102sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m                      labels\u001b[39m=\u001b[39;49mb_labels,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/coding/pythonnew/POLHATE-project/src/notebooks/3.0-deep-learning-models.ipynb#Y102sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m                     return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/coding/pythonnew/POLHATE-project/src/notebooks/3.0-deep-learning-models.ipynb#Y102sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m# Accumulate the training loss over all of the batches so that we can calculate the average loss at the end, \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/coding/pythonnew/POLHATE-project/src/notebooks/3.0-deep-learning-models.ipynb#Y102sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39m# `loss` is a tensor containing a single value; the `.item()` function just returns the Python value from the tensor.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/coding/pythonnew/POLHATE-project/src/notebooks/3.0-deep-learning-models.ipynb#Y102sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m total_train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32me:\\coding\\pythonnew\\POLHATE-project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\coding\\pythonnew\\POLHATE-project\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1206\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1199\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1200\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1201\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1202\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1206\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[0;32m   1207\u001b[0m     input_ids,\n\u001b[0;32m   1208\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1209\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1210\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1211\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1212\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1213\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1214\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1215\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1216\u001b[0m )\n\u001b[0;32m   1217\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1218\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32me:\\coding\\pythonnew\\POLHATE-project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\coding\\pythonnew\\POLHATE-project\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:848\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    839\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    841\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    842\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    843\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    846\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    847\u001b[0m )\n\u001b[1;32m--> 848\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    849\u001b[0m     embedding_output,\n\u001b[0;32m    850\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    851\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    852\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    853\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    854\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    855\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    856\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    857\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    858\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    859\u001b[0m )\n\u001b[0;32m    860\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    861\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\coding\\pythonnew\\POLHATE-project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\coding\\pythonnew\\POLHATE-project\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    515\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    516\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    517\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    521\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    522\u001b[0m     )\n\u001b[0;32m    523\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 524\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    525\u001b[0m         hidden_states,\n\u001b[0;32m    526\u001b[0m         attention_mask,\n\u001b[0;32m    527\u001b[0m         layer_head_mask,\n\u001b[0;32m    528\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    529\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    530\u001b[0m         past_key_value,\n\u001b[0;32m    531\u001b[0m         output_attentions,\n\u001b[0;32m    532\u001b[0m     )\n\u001b[0;32m    534\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    535\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32me:\\coding\\pythonnew\\POLHATE-project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\coding\\pythonnew\\POLHATE-project\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:409\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    398\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    399\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    406\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    407\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    408\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    410\u001b[0m         hidden_states,\n\u001b[0;32m    411\u001b[0m         attention_mask,\n\u001b[0;32m    412\u001b[0m         head_mask,\n\u001b[0;32m    413\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    414\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    415\u001b[0m     )\n\u001b[0;32m    416\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    418\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32me:\\coding\\pythonnew\\POLHATE-project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\coding\\pythonnew\\POLHATE-project\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:336\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    327\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    328\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    334\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    335\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 336\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    337\u001b[0m         hidden_states,\n\u001b[0;32m    338\u001b[0m         attention_mask,\n\u001b[0;32m    339\u001b[0m         head_mask,\n\u001b[0;32m    340\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    341\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    342\u001b[0m         past_key_value,\n\u001b[0;32m    343\u001b[0m         output_attentions,\n\u001b[0;32m    344\u001b[0m     )\n\u001b[0;32m    345\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    346\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32me:\\coding\\pythonnew\\POLHATE-project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\coding\\pythonnew\\POLHATE-project\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:272\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     attention_probs \u001b[39m=\u001b[39m attention_probs \u001b[39m*\u001b[39m head_mask\n\u001b[1;32m--> 272\u001b[0m context_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(attention_probs, value_layer)\n\u001b[0;32m    274\u001b[0m context_layer \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m    275\u001b[0m new_context_layer_shape \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_head_size,)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 3.00 GiB total capacity; 2.42 GiB already allocated; 0 bytes free; 2.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# This training code is based on the `run_glue.py` script here:\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, validation accuracy, f1 score and timings.\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print('')\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes:\n",
    "    \n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    \n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    \n",
    "    # `dropout` and `batchnorm` layers behave differently during training vs. test ,\n",
    "    # source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the device(gpu in our case) using the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        \n",
    "        b_input_ids = batch[0].to(device).to(torch.int64)\n",
    "        b_input_mask = batch[1].to(device).to(torch.int64)\n",
    "        b_labels = batch[2].to(device).to(torch.int64)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a backward pass. PyTorch doesn't do this automatically because accumulating the gradients is 'convenient while training RNNs'. \n",
    "        # Source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "        \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is down here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers BertForSequenceClassification.\n",
    "        \n",
    "        # It returns different numbers of parameters depending on what arguments given and what flags are set. For our useage here, it returns the loss (because we provided labels),\n",
    "        # And the 'logits' (the model outputs prior to activation.)\n",
    "        \n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels,\n",
    "                            return_dict=False )\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end, \n",
    "        # `loss` is a tensor containing a single value; the `.item()` function just returns the Python value from the tensor.\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0 This is to help prevent the 'exploding gradients' problem.\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        \n",
    "        # The optimizer dictates the 'update rule'(How the parameters are modified based on their gradients, the learning rate, etc.)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    \n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print('')\n",
    "    print('  Average training loss: {0:.2f}'.format(avg_train_loss))\n",
    "    print('  Training epcoh took: {:}'.format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on our validation set.\n",
    "\n",
    "    print('')\n",
    "    print('Running Validation...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables:\n",
    "    \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    total_eval_f1 = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch.\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        \n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n",
    "        \n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during the forward pass, since this is only needed for backprop (training part).\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the 'segment ids', which differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is down here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers BertForSequenceClassification.\n",
    "            # Get the 'logits' output by the model. The 'logits' are the output values prior to applying an activation function like the softmax.\n",
    "            \n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        \n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU:\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches:\n",
    "        \n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        total_eval_f1 += flat_f1(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print('  Accuracy: {0:.2f}'.format(avg_val_accuracy))\n",
    "    \n",
    "    # Report the final f1 score for this validation run.\n",
    "    \n",
    "    avg_val_f1 = total_eval_f1 / len(validation_dataloader)\n",
    "    print('  F1: {0:.2f}'.format(avg_val_f1))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    \n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Measure how long the validation run took:\n",
    "    \n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print('  Validation Loss: {0:.2f}'.format(avg_val_loss))\n",
    "    print('  Validation took: {:}'.format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    \n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Val_F1' : avg_val_f1,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print('')\n",
    "print('Training complete!')\n",
    "\n",
    "print('Total training took {:} (h:mm:ss)'.format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display floats with two decimal places.\n",
    "\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# Display the table.\n",
    "\n",
    "display(df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the plot size and font size:\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# Plot the learning curve:\n",
    "\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label='Training')\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label='Validation')\n",
    "\n",
    "# Label the plot:\n",
    "\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2241"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "075987e119c23eeae02c9b37e4e35603eda7102057cef8bf344d385806e7aa0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
